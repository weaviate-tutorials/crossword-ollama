{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook sets up a database and installs python modules to use with the crossword generator project. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Weaviate client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clear your current pip cache\n",
    "# !pip cache purge\n",
    "\n",
    "# Uncomment to upgrade pip\n",
    "# !pip install --upgrade pip\n",
    "\n",
    "# Install client from public released\n",
    "!pip3 install --no-cache -U \"weaviate-client==4.*\"\n",
    "\n",
    "# Check installed client version\n",
    "!pip show weaviate-client | grep Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install additional Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import tqdm progress monitor\n",
    "# !pip install tqdm\n",
    "\n",
    "# # Import the Ollama python client\n",
    "# !pip install ollama\n",
    "\n",
    "# # Import spacy named entity recognition for puzzle generation\n",
    "# #   It might take a few minutes to build spacy and setup the data\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Ollama\n",
    "\n",
    "Install an LLM and an embedding model.\n",
    "\n",
    "Ollam should start after you install it. To check if Ollama is running, open localhost:11434 in a browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to install Ollama\n",
    "# !ollama pull llama3         # The LLM\n",
    "# !ollama pull all-minilm     # For embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect the client to a local Weaviate instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "\n",
    "client = weaviate.connect_to_local()\n",
    "\n",
    "# Uncomment to check the connection\n",
    "client.is_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the Ollama module is enabled in Weaviate\n",
    "\n",
    "If the Ollama modules are not configured, enable the `text2vec-ollama` module and the `generative-ollama` module in your Weaviate [configuration file](/developers/weaviate/installation#configuration-files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_info = client.get_meta()\n",
    "if 'text2vec-ollama' not in meta_info[\"modules\"] :\n",
    "    print(\"Enable the text2vec-ollama module.\")\n",
    "\n",
    "if 'generative-ollama' not in meta_info[\"modules\"] :\n",
    "    print(\"Enable the generative-ollama module.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the collection name\n",
    "\n",
    "You will need a collection to store your data. This code lets you choose a collection name and cleans up any earlier versions if they exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the collection name\n",
    "collection_name = \"CrosswordPuzzles\"\n",
    "\n",
    "# Uncomment to remove old versions of this collection\n",
    "if (client.collections.exists(collection_name)):\n",
    "    client.collections.delete(collection_name)\n",
    "    print(f\"Removed old collection: {collection_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a collection\n",
    "\n",
    "The local collection holds some books from [Project Gutenberg](https://www.gutenberg.org/).\n",
    "\n",
    "This definition is very basic. When the books in the database are converted to vector embeddings below, they aren't given any meta-data to record as properties here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from weaviate.classes.config import Property, DataType, Configure\n",
    "\n",
    "# lets create the collection, specifing our base url accordingling\n",
    "collection = client.collections.create(\n",
    "    name=collection_name,\n",
    "    description=\"Source texts for puzzles\",\n",
    "    properties=[\n",
    "        Property(name=\"text\", data_type=DataType.TEXT),\n",
    "    ],\n",
    "    vectorizer_config=Configure.Vectorizer.text2vec_ollama(\n",
    "        api_endpoint=\"http://localhost:11434\",\n",
    "        model=\"all-minilm\"\n",
    "    ),\n",
    "    generative_config=Configure.Generative.ollama(\n",
    "        api_endpoint=\"http://localhost:11434\",\n",
    "        model=\"llama3\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# # Uncomment to check the collection definition\n",
    "# collection_definition = client.collections.export_config(collection_name)\n",
    "# print(f\"Name: {collection_definition.name}     Description: {collection_definition.description}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data into the collection\n",
    "\n",
    "Process some text files (books from Project Guttenberg) to use as project specific data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import ollama\n",
    "\n",
    "\n",
    "# Initiate spacy to process the files\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Get a list of the sources\n",
    "source_dir = \"../inputs/\"\n",
    "source_files = [f for f in os.listdir(source_dir) if os.path.isfile(source_dir + f)]\n",
    "\n",
    "for sf in source_files:\n",
    "    with open(source_dir + sf, 'r') as f:\n",
    "        sentences = []\n",
    "        header_flag = True\n",
    "        source_text = f.read()\n",
    "\n",
    "        # Split each source file into sentence-like strings\n",
    "        for s in nlp(source_text).sents:\n",
    "            if header_flag:\n",
    "                if str(s).startswith('*** START'):\n",
    "                    header_flag = False\n",
    "                continue\n",
    "            else:\n",
    "                if len(s) > 0:\n",
    "                    sentences.append(s)\n",
    "\n",
    "        # Create embeddings for the sentences\n",
    "        with collection.batch.dynamic() as batch:\n",
    "            for snt in sentences:\n",
    "                snt = str(snt)\n",
    "                response = ollama.embeddings(model=\"all-minilm\", prompt=snt)\n",
    "                embedding = response[\"embedding\"]\n",
    "                batch.add_object(\n",
    "                    properties = {\"text\": snt},\n",
    "                    vector = embedding,\n",
    "                )\n",
    "            # print(len(sentences))\n",
    "            # print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the data upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to print the number of objects\n",
    "# collection = client.collections.get(collection_name)\n",
    "# response = collection.aggregate.over_all(total_count=True)\n",
    "# print(f\"Collection size: {response.total_count}\")\n",
    "\n",
    "# # Uncomment to print the first 3 objects\n",
    "# response = collection.query.fetch_objects(\n",
    "#     limit=3,\n",
    "#     include_vector=True\n",
    "#     )\n",
    "# for o in response.objects:\n",
    "#     pp.pprint(o.properties)\n",
    "#     print(o.vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
